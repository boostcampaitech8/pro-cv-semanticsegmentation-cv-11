# multiplier 1.0, 1 에폭 warmup, 5 에폭 주기로 70% 감소
# train 640개, val 160개, batch_size train 2, val 2
# Class weights: tier_based (전략 2)
# Loss: BCE:Dice = 0.3:0.7 (단순 버전)
# Gradient accumulation: accum_steps=2
# ⚠️ 주의: SegFormer는 원래 MIT-B 계열만 지원하도록 설계되었습니다.
# HRNet-W64는 CNN 기반이라 구조적으로 맞지 않을 수 있습니다.
# 실제 작동 여부는 실행해봐야 확인 가능합니다.

# data 관련 설정
image_root: /data/ephemeral/home/dataset/train/DCM
label_root: /data/ephemeral/home/dataset/train/outputs_json

# 모델명 및 사전 학습 여부
# SMP의 SegFormer with HRNet-W64 encoder (실험적)
model_name: SegFormer
model_parameter:
  encoder_name: tu-hrnet_w64
  encoder_weights: imagenet
  classes: 29

# batch_size
train_batch_size: 2 # 1
val_batch_size: 2 # 1 
accum_steps: 2 # 4~8

# BatchNorm → GroupNorm 교체 설정 (batch_size=1일 때 gradient accumulation 사용 시 권장)
# replace_bn_with_gn: true  # BatchNorm을 GroupNorm으로 교체할지 여부
# gn_num_groups: 32         # GroupNorm의 그룹 수 (기본값: 32, 채널 수보다 클 수 없음)

# val_num_worker 지정
num_workers: 4

# image resize
image_size: &image_size 1024

# transform 관련
train_transform:
  Resize:
    width: *image_size
    height: *image_size
  ShiftScaleRotate:
    shift_limit: 0.05
    scale_limit: 0.05
    rotate_limit: 5
    p: 0.5
  HorizontalFlip:
    p: 0.5
  ElasticTransform:
    alpha: 1
    sigma: 50
    alpha_affine: 2
    p: 0.3
  CLAHE:
    clip_limit: 2.0
    tile_grid_size: [8, 8]
    p: 0.2
  RandomBrightnessContrast:
    brightness_limit: 0.1
    contrast_limit: 0.1
    p: 0.2

val_transform:
  Resize:
    width: *image_size
    height: *image_size

# 학습 관련 하이퍼파라미터
lr: 1e-3
weight_decay: 1e-6

max_epoch: &max_epoch 50

# loss 관련 설정
loss_name: Mixed

# combo loss 사용시
# BCE:Dice = 0.3:0.7
loss_parameter:
  losses:
    - name: BCEWithLogitsLoss
      weight: 0.3
      params: {}
    - name: DiceLoss
      weight: 0.7
      params:
        smooth: 1e-5

# class weights 설정 (전략 2: tier_based)
# 구간별 가중치: 0.90-0.93(3.0x), 0.93-0.96(2.5x), 0.96-0.97(2.0x), 0.97-0.98(1.5x), 0.98+(1.0x)
class_weights_path: configs/class_weights/class_weights_tier_based.json

# scheduler 관련 설정
scheduler_name: CosineAnnealingWarmupRestarts

# scheduler 필요한 parameter -> dict 형태로 작성
# CosineAnnealingWarmupRestarts 파라미터:
# - first_cycle_steps: 첫 번째 사이클의 총 스텝 수 (warmup 포함)
# - warmup_steps: 웜업 스텝 수
# - max_lr: 최대 learning rate (초기 lr과 동일)
# - min_lr: 최소 learning rate
# - cycle_mult: 사이클 배수 (1.0이면 동일한 사이클 반복)
# - gamma: 감쇠 계수 (0.7이면 각 사이클마다 lr이 70%로 감소)
# 
# 설정: train 640개, batch_size 2, 1 에폭당 320 steps
#       warmup 320 스텝(1 에폭) 후 5 에폭 주기로 70% 감소
#       first_cycle_steps = warmup(320) + 5 에폭(1600) = 1920 steps
scheduler_parameter:
  first_cycle_steps: 1920  # 320 * (1 + 5) = 1 에폭 warmup + 5 에폭 주기
  warmup_steps: 320  # 1 에폭 (640 / 2 = 320 steps)
  max_lr: 1e-3  # 초기 learning rate
  min_lr: 1e-6  # 최소 learning rate
  cycle_mult: 1.0  # 사이클 배수 (1.0이면 동일한 사이클 반복)
  gamma: 0.7  # 감쇠 계수 (각 사이클마다 lr이 70%로 감소)


# optimizer 관련 설정
optimizer_name: adamw

# random seed값
seed: 21

# validation 관련 인자
val_fold: 0
val_interval: 1
threshold: 0.5

# checkpoint 저장 경로
save_dir: /data/ephemeral/home/pro-cv-semanticsegmentation-cv-11/checkpoints/SegFormer_HRNet_W64
checkpoint_name_format: "segformer_hrnet_w64_cosinewarmup_tier_weights_smp_amp_grad-best_{epoch}epoch_{dice_score:.4f}.pt"

# wandb
team_name: cv_11
project_name: cv-11-SEG
auto_run_name: true  # 자동으로 run name 생성
run_name_decoder: segformer
run_name_backbone: hrnet_w64
run_name_exp_id: classwise_weights_smp_amp_grad
# experiment_detail: segformer_hrnet_w64_cosinewarmup_tier_weights_simple  # auto_run_name이 false일 때만 사용

